{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Learning | Assignment**"
      ],
      "metadata": {
        "id": "t1euqVpvGCCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "Ans -  Ensemble Learning is a technique where multiple models (weak learners) are combined to make better predictions.\n",
        "\n",
        "**Key idea:** Combining diverse models reduces errors, increases accuracy, and improves generalization compared to a single model.\n"
      ],
      "metadata": {
        "id": "wD4KC0yeGCFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Ans - Bagging: Trains models in parallel on different random subsets of data to reduce variance (e.g., Random Forest).\n",
        "\n",
        "Boosting: Trains models sequentially, where each model focuses on correcting the errors of the previous one to reduce bias (e.g., AdaBoost, XGBoost)."
      ],
      "metadata": {
        "id": "OkTXDTLIRV4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "Ans - Bootstrap sampling is randomly selecting data points with replacement to create multiple subsets.\n",
        "Role in Bagging: It ensures each model is trained on slightly different data, increasing diversity and reducing overfitting in methods like Random Forest."
      ],
      "metadata": {
        "id": "ZQD7iS_lRnj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Ans - OOB samples: Data points not included in a bootstrap sample for a given model.\n",
        "\n",
        "OOB score: Performance is measured on these unused samples, providing a built-in validation method without needing a separate test set."
      ],
      "metadata": {
        "id": "ot4tgMCBR67Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "Ans - Decision Tree: Feature importance is based on how much each feature reduces impurity at its splits.\n",
        "\n",
        "Random Forest: Importance is averaged over many trees, giving more stable, reliable, and less biased results."
      ],
      "metadata": {
        "id": "HwmtIiTpSMOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "_B1qSmXISWGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Feature importance\n",
        "importances = model.feature_importances_\n",
        "feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "# Print top 5 features\n",
        "print(\"Top 5 important features:\")\n",
        "print(feat_imp.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4EhqR2wSf5h",
        "outputId": "1863faa6-e470-4682-d8bf-5ebf37787bf3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 important features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "X9O97LmhSrZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Bagging Classifier with Decision Trees\n",
        "bagging = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_pred = bagging.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmrSZgmSS7g5",
        "outputId": "c8f405a0-9190-41b8-cc16-4b07c6996a80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n"
      ],
      "metadata": {
        "id": "Lj4wClfySrWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Random Forest with GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 3, 5, 7]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42),\n",
        "                    param_grid,\n",
        "                    cv=5,\n",
        "                    scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0bV-B49TC-G",
        "outputId": "ebc3f35d-a78f-45b3-a1db-aca047f739bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n"
      ],
      "metadata": {
        "id": "Mi2q8fxSSWJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Regressor with Decision Trees\n",
        "bagging = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_pred = bagging.predict(X_test)\n",
        "bag_mse = mean_squared_error(y_test, bag_pred)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os_QbIKtTRae",
        "outputId": "95e683a5-9cda-464b-ba0d-5e5bd2487e0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.25787382250585034\n",
            "Random Forest Regressor MSE: 0.25772464361712627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n"
      ],
      "metadata": {
        "id": "_DE_czM7TczD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans -\n",
        "\n",
        "1. **Choose Bagging or Boosting:**\n",
        "\n",
        "   * Start with **Bagging** (e.g., Random Forest) to reduce variance.\n",
        "   * Use **Boosting** (e.g., XGBoost, LightGBM) if the goal is to reduce bias and capture complex patterns.\n",
        "\n",
        "2. **Handle Overfitting:**\n",
        "\n",
        "   * Use regularization (max\\_depth, learning\\_rate in boosting).\n",
        "   * Limit tree size, tune n\\_estimators, apply early stopping.\n",
        "   * Use cross-validation to prevent model from memorizing data.\n",
        "\n",
        "3. **Select Base Models:**\n",
        "\n",
        "   * Decision Trees (common choice).\n",
        "   * Try Logistic Regression or Gradient Boosted Trees depending on data patterns.\n",
        "\n",
        "4. **Evaluate Performance (Cross-Validation):**\n",
        "\n",
        "   * Use **k-fold cross-validation** for robust performance estimation.\n",
        "   * Evaluate with metrics like AUC-ROC, Precision-Recall (important for imbalance).\n",
        "\n",
        "5. **Justification of Ensemble Learning:**\n",
        "\n",
        "   * Combines strengths of multiple models, reducing both variance and bias.\n",
        "   * Provides more reliable predictions, lowering risk of misclassifying good/bad borrowers.\n",
        "   * Improves decision-making by giving financial institutions a **more accurate, fair, and risk-aware loan approval system**.\n"
      ],
      "metadata": {
        "id": "xQI1l9n1UC04"
      }
    }
  ]
}